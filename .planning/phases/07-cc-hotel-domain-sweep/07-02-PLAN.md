---
phase: 07-cc-hotel-domain-sweep
plan: 02
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - workflows/discover_owners.py
autonomous: true

must_haves:
  truths:
    - "JSON-LD structured data is extracted from CC HTML when present (free, fast, high confidence)"
    - "Regex name+title patterns are extracted from CC HTML as fallback (free, fast, medium confidence)"
    - "LLM (Nova Micro) extracts owner names, titles, and roles from pages where structured extraction found nothing"
    - "Extracted people are converted to DecisionMaker objects with appropriate source tags and confidence scores"
    - "Results are flushed to hotel_decision_makers every 20 hotels, not accumulated until end"
  artifacts:
    - path: "workflows/discover_owners.py"
      provides: "Owner extraction (JSON-LD, regex, LLM) + incremental persistence + pipeline orchestration"
      contains: "llm_extract_owners"
  key_links:
    - from: "workflows/discover_owners.py"
      to: "bedrock-runtime (Nova Micro)"
      via: "boto3 converse API"
      pattern: "bedrock\\.converse.*nova-micro"
    - from: "workflows/discover_owners.py"
      to: "services/enrichment/owner_models.py"
      via: "DecisionMaker and OwnerEnrichmentResult imports"
      pattern: "from services\\.enrichment\\.owner_models import"
    - from: "workflows/discover_owners.py"
      to: "services/enrichment/repo.py"
      via: "batch_persist_results for incremental flush"
      pattern: "repo\\.batch_persist_results"
---

<objective>
Add owner extraction (JSON-LD + regex + LLM) and incremental persistence to the CC harvest pipeline, creating the complete data processing layer.

Purpose: Raw HTML from CC is useless without extraction. This plan adds the three-tier extraction strategy (structured data first, regex second, LLM last) and the incremental flush pattern so a crash at hotel 900 preserves the first 880.

Output: Updated `workflows/discover_owners.py` with extraction functions, persistence functions, and the main `discover_owners_cc()` pipeline function that ties harvest -> extract -> persist together.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-cc-hotel-domain-sweep/07-RESEARCH.md
@.planning/phases/07-cc-hotel-domain-sweep/07-01-SUMMARY.md

# Source files for patterns:
@workflows/enrich_contacts.py
@services/enrichment/owner_models.py
@services/enrichment/repo.py
@lib/owner_discovery/website_scraper.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add JSON-LD extraction, regex extraction, and LLM owner extraction</name>
  <files>workflows/discover_owners.py</files>
  <action>
Add three extraction functions to `discover_owners.py`. The strategy is: run JSON-LD first (free, highest confidence 0.9), then regex (free, medium confidence 0.7), then LLM only if neither found anything (costs money, confidence 0.65). This matches the recommendation in RESEARCH.md "Open Question 3."

**1. JSON-LD extraction -- adapt from `lib/owner_discovery/website_scraper.py` lines 113-161:**

```python
# Decision maker title keywords (for JSON-LD filtering)
DECISION_MAKER_TITLES = {
    'owner', 'proprietor', 'director', 'managing director',
    'general manager', 'manager', 'gm', 'ceo', 'president',
    'principal', 'founder', 'co-founder', 'partner',
    'chief executive', 'chief operating', 'vice president',
    'innkeeper', 'hotelier',
}

def extract_json_ld_persons(html: str) -> list[DecisionMaker]:
    """Extract Person entities from JSON-LD structured data in HTML."""
```

Copy the implementation from `website_scraper.py` lines 113-161 (extract_json_ld_persons + _extract_persons_from_jsonld). Change the source tag from `"website_scrape"` to `"cc_website_jsonld"`. Keep confidence at 0.9.

**2. Regex name+title extraction -- adapt from `website_scraper.py` lines 164-190:**

First, copy the NAME_TITLE_PATTERNS from website_scraper.py (search for `NAME_TITLE_PATTERNS` near the top of that file). Then:

```python
def extract_name_title_regex(text: str) -> list[DecisionMaker]:
    """Extract name+title combinations using regex patterns."""
```

Copy the implementation from website_scraper.py lines 164-190. Change source tag from `"website_scrape"` to `"cc_website_regex"`. Keep confidence at 0.7.

**3. LLM owner extraction -- new function based on RESEARCH.md code example:**

```python
_bedrock_client = None

def _get_bedrock():
    global _bedrock_client
    if _bedrock_client is None:
        import boto3
        _bedrock_client = boto3.client('bedrock-runtime', region_name=AWS_REGION)
    return _bedrock_client

_llm_sem = asyncio.Semaphore(30)  # Bedrock throttles above ~30 concurrent

async def llm_extract_owners(text: str, hotel_name: str) -> list[dict]:
    """Use Nova Micro to extract owner/GM info from page text.

    Returns list of dicts: [{"name": "First Last", "title": "Title", "role": "owner|general_manager|..."}]
    """
    prompt = f"""Extract all hotel owners, general managers, and key decision makers from this text.
Hotel: {hotel_name}

Text:
{text}

Rules:
- Each person MUST have both first name AND surname (e.g. "John Smith", not just "John")
- Do NOT return company names, trust names, or business entities as person names
- For each person, identify their role: owner, general_manager, director, manager, or other
- Only include people clearly associated with this hotel/property
- Respond with ONLY a JSON array, no explanation

JSON format: [{{"name":"First Last","title":"Their Title","role":"owner|general_manager|director|manager|other"}}]
If no people found, respond with exactly: []"""

    async with _llm_sem:
        for attempt in range(3):
            try:
                bedrock = _get_bedrock()
                resp = await asyncio.to_thread(
                    bedrock.converse,
                    modelId=BEDROCK_MODEL_ID,
                    messages=[{"role": "user", "content": [{"text": prompt}]}],
                    inferenceConfig={"maxTokens": 500, "temperature": 0.0},
                )
                content = resp["output"]["message"]["content"][0]["text"].strip()
                # Parse JSON -- same cleanup as enrich_contacts.py lines 1404-1410
                if content.startswith("```"):
                    content = re.sub(r'^```\w*\n?', '', content)
                    content = re.sub(r'\n?```$', '', content)
                json_match = re.search(r'\[.*\]', content, re.DOTALL)
                if json_match:
                    content = json_match.group(0)
                return json.loads(content)
            except Exception as e:
                err_str = str(e)
                if attempt < 2 and ("throttl" in err_str.lower() or "429" in err_str or "Too Many" in err_str):
                    wait = 2 ** (attempt + 1)
                    logger.warning(f"Bedrock throttled, retry in {wait}s...")
                    await asyncio.sleep(wait)
                    continue
                logger.warning(f"LLM extraction error: {e}")
                return []
    return []
```

**4. Conversion function -- turn LLM results into DecisionMaker objects:**

```python
def llm_results_to_decision_makers(results: list[dict], source_url: str) -> list[DecisionMaker]:
    """Convert LLM extraction results to DecisionMaker objects."""
    dms = []
    for r in results:
        name = (r.get("name") or "").strip()
        if not name or " " not in name:
            continue  # Skip first-name-only
        # Filter out entity names (companies, trusts, etc.)
        if re.search(ENTITY_RE_STR, name, re.IGNORECASE):
            continue
        title = (r.get("title") or r.get("role") or "").strip()
        if not title:
            title = "Unknown Role"
        dms.append(DecisionMaker(
            full_name=name,
            title=title.title(),
            sources=["cc_website_llm"],
            confidence=0.65,
            raw_source_url=source_url,
        ))
    return dms
```

**5. Combined extraction function -- orchestrates the three-tier strategy:**

```python
async def extract_owners_from_page(html: str, url: str, hotel_name: str) -> list[DecisionMaker]:
    """Extract owner/manager names from a single page using three-tier strategy.

    1. JSON-LD structured data (free, 0.9 confidence)
    2. Regex name+title patterns (free, 0.7 confidence)
    3. LLM extraction (costs money, 0.65 confidence) -- only if 1+2 found nothing
    """
    # Tier 1: JSON-LD (structured data in HTML)
    dms = extract_json_ld_persons(html)
    if dms:
        # Set source URL on JSON-LD results
        for dm in dms:
            dm.raw_source_url = url
        logger.debug(f"JSON-LD: {len(dms)} persons from {url}")
        return dms

    # Tier 2: Regex patterns on cleaned text
    cleaned = _clean_text_for_llm(html)
    dms = extract_name_title_regex(cleaned)
    if dms:
        for dm in dms:
            dm.raw_source_url = url
            dm.sources = ["cc_website_regex"]
        logger.debug(f"Regex: {len(dms)} persons from {url}")
        return dms

    # Tier 3: LLM extraction (only if structured methods found nothing)
    if len(cleaned) < 50:
        return []  # Too little text for LLM
    truncated = cleaned[:20000]  # Nova Micro context limit
    results = await llm_extract_owners(truncated, hotel_name)
    dms = llm_results_to_decision_makers(results, url)
    if dms:
        logger.debug(f"LLM: {len(dms)} persons from {url}")
    return dms
```

IMPORTANT: Also add `import boto3` to the imports section at the top of the file (inside the _get_bedrock function is fine too, as a lazy import, matching the existing enrich_contacts.py pattern where boto3 is imported lazily). Also make sure `from html import unescape as html_unescape` is in the imports if not already present (needed by _clean_text_for_llm).
  </action>
  <verify>
Run `uv run python3 -c "from workflows.discover_owners import extract_owners_from_page, llm_extract_owners, extract_json_ld_persons, extract_name_title_regex, llm_results_to_decision_makers; print('All extraction functions OK')"` -- should succeed.
  </verify>
  <done>Three-tier extraction strategy (JSON-LD -> regex -> LLM) is implemented. LLM uses Nova Micro via Bedrock with retry on 429 and semaphore(30) concurrency limit. Entity names are filtered out. Results are converted to DecisionMaker objects with appropriate source tags and confidence scores.</done>
</task>

<task type="auto">
  <name>Task 2: Add incremental persistence and main pipeline orchestration function</name>
  <files>workflows/discover_owners.py</files>
  <action>
Add the incremental persistence and main pipeline function to `discover_owners.py`.

**1. Group pages by domain helper:**

```python
def group_pages_by_domain(pages: dict[str, str]) -> dict[str, dict[str, str]]:
    """Group fetched pages by domain.

    Returns {domain: {url: html, url2: html2, ...}}
    """
    grouped = defaultdict(dict)
    for url, html in pages.items():
        domain = _get_domain(url)
        if domain:
            grouped[domain][url] = html
    return dict(grouped)
```

**2. Main pipeline function -- `discover_owners_cc()`:**

This is the core orchestration. It wires together: load hotels -> extract domains -> CC harvest -> extract owners -> persist. Adapted from the skeleton in RESEARCH.md but with proper incremental flush.

```python
async def discover_owners_cc(args, cfg: dict) -> dict:
    """Main CC owner discovery pipeline.

    Returns stats dict: {hotels_processed, domains_queried, pages_fetched,
                         owners_found, owners_saved, hotels_with_owners}
    """
    from services.enrichment import repo

    stats = {
        'hotels_loaded': 0, 'domains_queried': 0,
        'pages_fetched': 0, 'owners_extracted': 0,
        'owners_saved': 0, 'hotels_with_owners': 0,
        'llm_calls': 0, 'jsonld_hits': 0, 'regex_hits': 0,
    }

    # 1. Load hotels
    conn = await asyncpg.connect(**DB_CONFIG)
    try:
        hotels = await load_hotels_for_cc_sweep(conn, cfg, limit=args.limit)
    finally:
        await conn.close()

    stats['hotels_loaded'] = len(hotels)
    logger.info(f"Loaded {len(hotels)} hotels for CC sweep")

    if not hotels:
        logger.info("No hotels to process")
        return stats

    if args.dry_run:
        # Just show what would be processed
        all_domains, domain_map = extract_hotel_domains(hotels)
        print(f"\nDRY RUN:")
        print(f"  Hotels: {len(hotels)}")
        print(f"  Unique domains: {len(all_domains)}")
        print(f"  CC indexes to query: {len(CC_INDEXES)}")
        print(f"  Total CC queries: {len(all_domains) * len(CC_INDEXES)}")
        return stats

    # 2. Extract unique domains
    all_domains, domain_map = extract_hotel_domains(hotels)
    stats['domains_queried'] = len(all_domains)
    logger.info(f"Extracted {len(all_domains)} unique domains from {len(hotels)} hotels")

    if not all_domains:
        logger.warning("No valid domains found")
        return stats

    # 3. CC Harvest (index query + WARC fetch)
    pages = await cc_harvest_owner_pages(all_domains)
    stats['pages_fetched'] = len(pages)
    logger.info(f"CC harvest: {len(pages)} pages from {len(all_domains)} domains")

    if not pages:
        logger.warning("No pages fetched from CC")
        return stats

    # 4. Group pages by domain
    pages_by_domain = group_pages_by_domain(pages)

    # 5. Extract owners + incremental persistence
    pending_buffer: list[OwnerEnrichmentResult] = []
    flush_lock = asyncio.Lock()

    async def _flush():
        nonlocal pending_buffer
        async with flush_lock:
            if not pending_buffer:
                return 0
            to_flush = pending_buffer
            pending_buffer = []
        if not args.apply:
            # Not persisting, just count
            count = sum(len(r.decision_makers) for r in to_flush)
            return count
        try:
            count = await repo.batch_persist_results(to_flush)
            stats['owners_saved'] += count
            logger.info(f"Flushed {len(to_flush)} hotels ({count} DMs saved, {stats['owners_saved']} total)")
            return count
        except Exception as e:
            logger.error(f"Flush failed: {e}")
            async with flush_lock:
                pending_buffer = to_flush + pending_buffer
            return 0

    hotels_processed = 0
    for domain, domain_pages in pages_by_domain.items():
        hotels_for_domain = domain_map.get(domain, [])
        if not hotels_for_domain:
            continue

        # Extract owners from all pages for this domain
        all_dms_for_domain: list[DecisionMaker] = []
        for url, html in domain_pages.items():
            dms = await extract_owners_from_page(html, url, hotels_for_domain[0]['name'])
            all_dms_for_domain.extend(dms)

            # Track extraction method stats
            for dm in dms:
                if 'jsonld' in dm.sources[0]:
                    stats['jsonld_hits'] += 1
                elif 'regex' in dm.sources[0]:
                    stats['regex_hits'] += 1
                elif 'llm' in dm.sources[0]:
                    stats['llm_calls'] += 1

        stats['owners_extracted'] += len(all_dms_for_domain)

        # Create result for each hotel on this domain
        for hotel in hotels_for_domain:
            result = OwnerEnrichmentResult(
                hotel_id=hotel['hotel_id'],
                domain=domain,
                decision_makers=all_dms_for_domain,
                layers_completed=LAYER_WEBSITE,
            )
            if result.found_any:
                stats['hotels_with_owners'] += 1

            async with flush_lock:
                pending_buffer.append(result)
                should_flush = len(pending_buffer) >= FLUSH_INTERVAL

            if should_flush:
                await _flush()

            hotels_processed += 1

    # Final flush
    await _flush()

    # Summary
    logger.info(
        f"\nCC Sweep complete:\n"
        f"  Hotels processed: {hotels_processed}\n"
        f"  Domains queried: {stats['domains_queried']}\n"
        f"  Pages fetched: {stats['pages_fetched']}\n"
        f"  Owners extracted: {stats['owners_extracted']}\n"
        f"  Hotels with owners: {stats['hotels_with_owners']}\n"
        f"  Extraction: JSON-LD={stats['jsonld_hits']}, "
        f"Regex={stats['regex_hits']}, LLM={stats['llm_calls']}\n"
        f"  Owners saved to DB: {stats['owners_saved']}"
    )

    return stats
```

This function:
- Loads hotels from DB using SOURCE_CONFIGS
- Handles --dry-run by showing what would be processed and returning early
- Respects --apply: only writes to DB if --apply is set
- Flushes every FLUSH_INTERVAL (20) hotels using repo.batch_persist_results()
- Tracks detailed stats for audit reporting
- Uses the same OwnerEnrichmentResult model and batch_persist_results() from the existing codebase
- Sets layers_completed=LAYER_WEBSITE (bitmask 8) on results so the enrichment status table tracks CC sweep as the "website" layer
  </action>
  <verify>
Run `uv run python3 -c "from workflows.discover_owners import discover_owners_cc, group_pages_by_domain; print('Pipeline functions OK')"` -- should succeed.
  </verify>
  <done>Main pipeline function `discover_owners_cc()` exists wiring harvest -> extract -> persist with incremental flush every 20 hotels. Respects --apply and --dry-run flags. Tracks extraction method stats (JSON-LD, regex, LLM). Uses existing batch_persist_results() from repo.py.</done>
</task>

</tasks>

<verification>
1. `uv run python3 -c "import workflows.discover_owners"` succeeds
2. All extraction functions importable: `extract_json_ld_persons`, `extract_name_title_regex`, `llm_extract_owners`, `extract_owners_from_page`
3. Pipeline function importable: `discover_owners_cc`, `group_pages_by_domain`
4. `llm_extract_owners` uses `asyncio.Semaphore(30)` for Bedrock concurrency
5. `extract_owners_from_page` tries JSON-LD first, then regex, then LLM (in that order)
6. `discover_owners_cc` calls `repo.batch_persist_results()` for incremental flush
7. `FLUSH_INTERVAL` is 20
</verification>

<success_criteria>
- Three-tier extraction: JSON-LD (0.9) -> regex (0.7) -> LLM (0.65)
- LLM only fires when structured extraction found nothing (cost savings)
- Entity name filter prevents company names from becoming DecisionMakers
- Incremental persistence flushes every 20 hotels via batch_persist_results()
- Pipeline orchestrates: load -> harvest -> extract -> persist
- --dry-run shows count without processing, --apply controls DB writes
</success_criteria>

<output>
After completion, create `.planning/phases/07-cc-hotel-domain-sweep/07-02-SUMMARY.md`
</output>
