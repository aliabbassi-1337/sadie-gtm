---
phase: 07-cc-hotel-domain-sweep
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - workflows/discover_owners.py
autonomous: true

must_haves:
  truths:
    - "CC Index queries return NDJSON entries for hotel domain pages across 3 CC indexes"
    - "WARC records are fetched and decompressed into usable HTML"
    - "Only owner-relevant pages are kept (about, team, management, etc.) while junk is filtered"
    - "Aggregator domains (booking.com, expedia.com) are excluded from processing"
  artifacts:
    - path: "workflows/discover_owners.py"
      provides: "CC harvest infrastructure: index query, WARC fetch, URL filtering, domain extraction"
      min_lines: 250
  key_links:
    - from: "workflows/discover_owners.py"
      to: "CF Worker /batch endpoint"
      via: "_proxy_batch() sending CC Index queries"
      pattern: "_proxy_batch.*session.*batch"
    - from: "workflows/discover_owners.py"
      to: "data.commoncrawl.org"
      via: "WARC range requests through CF Worker /batch"
      pattern: "CC_WARC_BASE.*filename"
---

<objective>
Create the CC harvest infrastructure for owner discovery: batch CC Index querying across multiple indexes, WARC record fetching and decompression, and owner-relevant URL filtering.

Purpose: This is the data acquisition layer. Without CC harvest, there is no HTML to extract owner names from. The harvest must process all hotel domains in bulk (not per-hotel) via CF Worker /batch for speed and IP rotation.

Output: `workflows/discover_owners.py` with working CC harvest functions that accept a set of domains and return `{url: html}` of owner-relevant pages.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-cc-hotel-domain-sweep/07-RESEARCH.md

# Primary source files to reuse patterns from:
@workflows/enrich_contacts.py
@services/enrichment/owner_models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create discover_owners.py with environment, constants, and utility functions</name>
  <files>workflows/discover_owners.py</files>
  <action>
Create `workflows/discover_owners.py` with the following sections, closely mirroring the structure of `enrich_contacts.py`:

**Module docstring:**
```
Owner discovery via Common Crawl â€” find hotel owners, GMs, and decision makers.

Discovers NEW people by extracting owner/manager names from CC-cached hotel
website pages (/about, /team, /management, etc.). This is different from
enrich_contacts.py which finds contact info for EXISTING known people.

Pipeline:
  Phase 1: Query CC Indexes for hotel domain pages (batch via CF Worker)
  Phase 2: Fetch WARC records and decompress HTML (batch via CF Worker)
  Phase 3: Extract owner names via JSON-LD, regex, then LLM (Nova Micro)
  Phase 4: Persist results to hotel_decision_makers (incremental flush)

Usage:
    uv run python3 workflows/discover_owners.py --source big4 --apply --limit 100
    uv run python3 workflows/discover_owners.py --source big4 --audit
    uv run python3 workflows/discover_owners.py --source big4 --dry-run
```

**Imports:** Copy the import block from `enrich_contacts.py` lines 24-49. Include: argparse, asyncio, base64, gzip, json, os, re, sys, collections.defaultdict, pathlib.Path, typing.Optional, urllib.parse (urlparse, quote). External: asyncpg, aiohttp, loguru.logger. Do NOT import crawl4ai or httpx (not needed for CC-only pipeline). Add: `from services.enrichment.owner_models import DecisionMaker, OwnerEnrichmentResult, LAYER_WEBSITE`.

**Environment setup:** Copy `_read_env()` and `_parse_db_config()` from `enrich_contacts.py` lines 52-94 EXACTLY. Copy the env variable extraction: DB_CONFIG, AWS_REGION, BEDROCK_MODEL_ID, CF_WORKER_URL, CF_WORKER_AUTH (lines 94-99).

**Constants:**
- `CC_INDEXES` -- copy from enrich_contacts.py lines 105-109 (same 3 indexes)
- `CC_WARC_BASE` -- copy from enrich_contacts.py line 110
- `OWNER_PATHS` -- new set, superset of CONTACT_PATHS:
  ```python
  OWNER_PATHS = {
      'about', 'about-us', 'our-story', 'who-we-are', 'company',
      'team', 'our-team', 'the-team', 'leadership', 'leadership-team',
      'management', 'executive-team', 'board', 'board-of-directors',
      'directors', 'people', 'our-people', 'meet-the-team', 'staff',
      'our-hotel', 'the-hotel', 'hotel', 'ownership', 'proprietor',
      'contact', 'contact-us',
  }
  ```
- `SKIP_DOMAINS` -- aggregator/OTA domains to exclude:
  ```python
  SKIP_DOMAINS = {
      'booking.com', 'expedia.com', 'hotels.com', 'tripadvisor.com',
      'agoda.com', 'trivago.com', 'kayak.com', 'priceline.com',
      'wotif.com', 'lastminute.com', 'hostelworld.com',
      'airbnb.com', 'vrbo.com', 'stayz.com.au',
      'facebook.com', 'instagram.com', 'twitter.com', 'linkedin.com',
      'google.com', 'youtube.com', 'tiktok.com', 'x.com',
  }
  ```
- `ENTITY_RE_STR` -- copy from enrich_contacts.py line 112-117 (for filtering out business entity names from LLM results)
- `FLUSH_INTERVAL = 20`

**SOURCE_CONFIGS:** Copy from enrich_contacts.py lines 196-211, same structure. These define where/join/label/country for hotel queries. The discover_owners workflow needs the same hotel filtering.

**Utility functions:**
- `_proxy_headers()` -- copy from enrich_contacts.py line 239-241
- `_proxy_url()` -- copy from enrich_contacts.py lines 244-249
- `_proxy_batch()` -- copy from enrich_contacts.py lines 252-295 EXACTLY (this is critical infrastructure, do not modify)
- `_get_domain()` -- copy from enrich_contacts.py lines 298-303
- `_clean_text_for_llm()` -- copy from enrich_contacts.py lines 306-316
- `_is_owner_url()` -- new function, adapted from `_is_contact_url()`:
  ```python
  def _is_owner_url(url: str) -> bool:
      """Check if URL looks like an owner/about/team page (or homepage)."""
      path = urlparse(url).path.lower().strip('/')
      if not path:
          return True  # Homepage often has owner info
      return any(p in path for p in OWNER_PATHS)
  ```

Do NOT add the LLM extraction, persistence, pipeline, or CLI sections yet -- those come in Plans 02 and 03.
  </action>
  <verify>
Run `uv run python3 -c "import workflows.discover_owners as m; print('OK:', hasattr(m, '_proxy_batch'), hasattr(m, '_is_owner_url'), hasattr(m, 'OWNER_PATHS'), hasattr(m, 'SKIP_DOMAINS'))"` -- should print `OK: True True True True`
  </verify>
  <done>discover_owners.py exists with all environment setup, constants, SOURCE_CONFIGS, and utility functions. Module imports without error.</done>
</task>

<task type="auto">
  <name>Task 2: Add CC harvest function (index query + WARC fetch + HTML decompression)</name>
  <files>workflows/discover_owners.py</files>
  <action>
Add the CC harvest function to `discover_owners.py`, adapted from `enrich_contacts.py` `cc_harvest()` (lines 593-714). The key differences from enrich_contacts.py:

1. Input is a `set[str]` of domains (not `list[DMTarget]`)
2. URL filtering uses `_is_owner_url()` instead of `_is_contact_url()`
3. Returns `dict[str, str]` mapping `{page_url: html_content}`

**Function signature:**
```python
async def cc_harvest_owner_pages(all_domains: set[str]) -> dict[str, str]:
    """Batch CC Index query + WARC fetch for owner-relevant pages.

    Phase 1: Query all CC indexes for all domains via CF Worker /batch
    Phase 2: Filter to owner-relevant URLs (about, team, management, etc.)
    Phase 3: Fetch WARC records and decompress to HTML

    Returns {page_url: html_content}
    """
```

**Phase 1 -- CC Index queries:**
Copy the pattern from enrich_contacts.py lines 615-659. For each CC index, build batch requests for each domain:
```python
cc_url = f"{idx_url}?url={quote(f'*.{domain}/*', safe='')}&output=json&limit=200"
```
Fire all index batches concurrently via `asyncio.gather(*[_proxy_batch(session, batch) for batch in per_index_batches])`.

Parse NDJSON responses. For each entry:
- Check `'html' in (entry.get('mime', '') + entry.get('mime-detected', ''))`
- Check `_is_owner_url(entry.get('url', ''))` (uses OWNER_PATHS, NOT CONTACT_PATHS)
- Track `domains_hit` set

Deduplicate by URL (keep first = newest index).

**Phase 2 -- WARC fetches:**
Copy the pattern from enrich_contacts.py lines 664-712. For each unique entry:
- Skip entries with `length > 500_000` (huge pages)
- Build range requests: `warc_url = f"{CC_WARC_BASE}{filename}"`, `range_header = f"bytes={offset}-{offset+length-1}"`
- Fetch via `_proxy_batch(session, warc_requests, chunk_size=200)`

**Phase 3 -- Decompress WARC to HTML:**
Copy the decompression from enrich_contacts.py lines 687-712:
- base64 decode if binary, gzip decompress, split by `\r\n\r\n` (3 parts), take part[2] as HTML
- Decode with entry encoding, fallback to utf-8 replace
- Skip pages with less than 100 chars HTML

Log summary: domains hit, pages fetched, errors.

Use `aiohttp.TCPConnector(limit=0, ttl_dns_cache=300)` for the session (same as enrich_contacts.py line 615).

**Also add a hotel loading function:**
```python
async def load_hotels_for_cc_sweep(conn, cfg: dict, limit: int = None) -> list[dict]:
    """Load hotels with website domains for CC owner discovery.

    Excludes hotels without websites and aggregator-domain hotels.
    """
    jc = cfg.get("join") or ""
    wc = cfg["where"]
    rows = await conn.fetch(
        f"SELECT h.id AS hotel_id, h.name, h.website"
        f" FROM sadie_gtm.hotels h"
        f" {jc}"
        f" WHERE ({wc})"
        f"  AND h.website IS NOT NULL AND h.website != ''"
        f" ORDER BY h.id"
        f" {f'LIMIT {limit}' if limit else ''}",
    )
    return [dict(r) for r in rows]
```

**And a domain extraction helper:**
```python
def extract_hotel_domains(hotels: list[dict]) -> tuple[set[str], dict[str, list[dict]]]:
    """Extract unique domains from hotels, mapping domain -> hotels.

    Excludes aggregator/OTA domains.
    Returns (all_domains, domain_to_hotels_map)
    """
    all_domains = set()
    domain_map = defaultdict(list)  # domain -> [hotel dicts]
    for h in hotels:
        domain = _get_domain(h['website'])
        if domain and not any(domain == s or domain.endswith('.' + s) for s in SKIP_DOMAINS):
            all_domains.add(domain)
            domain_map[domain].append(h)
    return all_domains, domain_map
```
  </action>
  <verify>
Run `uv run python3 -c "import workflows.discover_owners as m; print('harvest:', hasattr(m, 'cc_harvest_owner_pages'), 'load:', hasattr(m, 'load_hotels_for_cc_sweep'), 'extract:', hasattr(m, 'extract_hotel_domains'))"` -- should print `harvest: True load: True extract: True`
  </verify>
  <done>cc_harvest_owner_pages() function exists and follows the exact same CC Index query + WARC fetch pattern as enrich_contacts.py, but with OWNER_PATHS filtering. load_hotels_for_cc_sweep() and extract_hotel_domains() functions exist for hotel loading and domain extraction.</done>
</task>

</tasks>

<verification>
1. `uv run python3 -c "import workflows.discover_owners"` succeeds without import errors
2. Module has all expected functions: `_proxy_batch`, `_is_owner_url`, `cc_harvest_owner_pages`, `load_hotels_for_cc_sweep`, `extract_hotel_domains`
3. Module has all expected constants: `CC_INDEXES`, `CC_WARC_BASE`, `OWNER_PATHS`, `SKIP_DOMAINS`, `SOURCE_CONFIGS`
4. `OWNER_PATHS` contains at least: about, team, management, leadership, ownership, our-hotel
5. `SKIP_DOMAINS` contains at least: booking.com, expedia.com, tripadvisor.com, airbnb.com
</verification>

<success_criteria>
- discover_owners.py exists with ~250-350 lines
- CC harvest function follows the exact enrich_contacts.py pattern (index query via CF Worker /batch, WARC fetch, gzip decompress)
- Owner URL filtering uses OWNER_PATHS superset (not the narrower CONTACT_PATHS)
- Aggregator domains are excluded
- Module imports cleanly
</success_criteria>

<output>
After completion, create `.planning/phases/07-cc-hotel-domain-sweep/07-01-SUMMARY.md`
</output>
