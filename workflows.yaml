# Workflow definitions for Sadie GTM
#
# Pipeline: Ingest → Scrape (local) → Enqueue → Detect (EC2) → Enrich (EC2) → Launch (EC2) → Export
#
# Status values:
#   -3 = duplicate (same placeId/location/name)
#   -2 = location_mismatch (rejected)
#   -1 = no_booking_engine (rejected)
#    0 = pending (in pipeline)
#    1 = launched (live lead)

# =============================================================================
# LOCAL WORKFLOWS (run on your machine)
# =============================================================================
local:
  # Step 1: Ingest region polygons (one-time per state)
  ingest_regions:
    description: Fetch city polygons from OSM and save to database
    command: uv run python -m workflows.ingest_regions --state FL
    # Run once per state to set up scraping regions

  # Step 2: Scrape hotels in polygon regions
  scrape_polygon:
    description: Scrape hotels from Serper for polygon regions
    command: uv run python -m workflows.scrape_polygon --state FL
    # Scrapes all ingested regions for a state
    # Use --only "Miami" "Orlando" to scrape specific cities
    # Use --estimate to preview cost before scraping

  # Step 3: Deduplicate scraped hotels
  deduplicate:
    description: Mark duplicate hotels (placeId → location → name)
    command: uv run python -m workflows.deduplicate
    # Use --dry-run to preview, --stats for stats only

  # Step 4: Queue hotels for detection
  enqueue_detection:
    description: Queue scraped hotels to SQS for detection workers
    command: uv run python workflows/enqueue_detection.py --limit 500

  # Step 5: Export final leads
  export:
    description: Export leads to Excel and upload to S3
    command: uv run python workflows/export.py --all-states

  # Utility: Check pipeline status
  status:
    description: Check pipeline status (pending, processing, launched counts)
    command: uv run python workflows/launcher.py status

# =============================================================================
# EC2 WORKFLOWS (auto-start on instance boot via systemd/cron)
# =============================================================================
# These run automatically when you start EC2 instances
# Configure once in AMI, then just start/stop instances as needed
ec2:
  detection_consumer:
    description: Poll SQS and process hotel detection (parallelizable)
    command: uv run python workflows/detection_consumer.py --concurrency 6
    type: systemd  # Runs continuously while instance is up

  enrichment_room_counts:
    description: Enrich hotels with room counts via Groq LLM (tier 1 only)
    command: uv run python workflows/enrichment.py room-counts --limit 100 --tier 1
    schedule: "*/10 * * * *"  # Every 10 min
    type: cron

  enrichment_proximity:
    description: Calculate nearest existing customer distance
    command: uv run python workflows/enrichment.py proximity --limit 200
    schedule: "*/5 * * * *"  # Every 5 min (fast, no API)
    type: cron

  launcher:
    description: Launch fully enriched hotels (mark as live)
    command: uv run python workflows/launcher.py launch-all
    schedule: "*/5 * * * *"  # Every 5 min
    type: cron
