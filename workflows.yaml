# Workflow definitions for Sadie GTM
#
# Pipeline: Ingest → Scrape (local) → Enqueue → Detect (EC2) → Enrich (EC2) → Launch (EC2) → Export
#
# Status values:
#   -3 = duplicate (same placeId/location/name)
#   -2 = location_mismatch (rejected)
#   -1 = no_booking_engine (rejected)
#    0 = pending (in pipeline)
#    1 = launched (live lead)

# =============================================================================
# LOCAL WORKFLOWS (run on your machine)
# =============================================================================
local:
  # Step 1: Ingest region polygons (one-time per state)
  ingest_regions:
    description: Fetch city polygons from OSM and save to database
    command: uv run python -m workflows.ingest_regions --state FL
    # Run once per state to set up scraping regions

  # Step 2: Scrape hotels in polygon regions
  scrape_polygon:
    description: Scrape hotels from Serper for polygon regions
    command: uv run python -m workflows.scrape_polygon --state FL
    # Scrapes all ingested regions for a state
    # Use --only "Miami" "Orlando" to scrape specific cities
    # Use --estimate to preview cost before scraping

  # Step 3: Deduplicate scraped hotels
  deduplicate:
    description: Mark duplicate hotels (placeId → location → name)
    command: uv run python -m workflows.deduplicate
    # Use --dry-run to preview, --stats for stats only

  # Step 4: Queue hotels for detection
  enqueue_detection:
    description: Queue scraped hotels to SQS for detection workers
    command: uv run python workflows/enqueue_detection.py --limit 500

  # Step 5: Export final leads
  export:
    description: Export leads to Excel and upload to S3
    command: uv run python workflows/export.py --state FL
    # Use --state FL for full state export
    # Use --city "Miami" --state FL for single city

  # Utility: Check pipeline status
  status:
    description: Check pipeline status (pending, processing, launched counts)
    command: uv run python workflows/launcher.py status

# =============================================================================
# EC2 WORKFLOWS (auto-start on instance boot via systemd/cron)
# =============================================================================
# These run automatically when you start EC2 instances
# Configure once in AMI, then just start/stop instances as needed
ec2:
  detection_consumer:
    description: Poll SQS and process hotel detection (parallelizable)
    command: uv run python workflows/detection_consumer.py --concurrency 6
    type: systemd  # Runs continuously while instance is up

  enqueue_detection:
    description: Auto-queue pending hotels for detection
    command: uv run python workflows/enqueue_detection.py --limit 500 --categories hotel motel --no-notify
    schedule: "*/5 * * * *"  # Every 5 min
    type: cron

  enrichment_room_counts:
    description: Enrich hotels with room counts via Groq LLM
    command: uv run python workflows/enrichment.py room-counts --limit 100
    schedule: "*/2 * * * *"  # Every 2 min
    type: cron

  enrichment_proximity:
    description: Calculate nearest existing customer distance
    command: uv run python workflows/enrichment.py proximity --limit 200
    schedule: "*/2 * * * *"  # Every 2 min
    type: cron

  launcher:
    description: Launch fully enriched hotels (mark as live)
    command: uv run python workflows/launcher.py launch-all
    schedule: "*/2 * * * *"  # Every 2 min
    type: cron

  # RMS Enrichment
  enrich_rms_enqueue:
    description: Enqueue RMS hotels needing enrichment
    command: uv run python workflows/enrich_rms_enqueue.py --limit 1000
    schedule: "0 */4 * * *"  # Every 4 hours
    type: cron

  enrich_rms_consumer:
    description: Scrape RMS booking pages for hotel data (name, email, phone, etc.)
    command: uv run python workflows/enrich_rms_consumer.py --concurrency 6
    type: systemd  # Runs continuously
