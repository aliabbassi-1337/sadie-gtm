# Workflow definitions for Sadie GTM
#
# Pipeline: Ingest → Scrape (local) → Enqueue → Detect (EC2) → Enrich (EC2) → Launch (EC2) → Export
#
# Status values:
#   -3 = duplicate (same placeId/location/name)
#   -2 = location_mismatch (rejected)
#   -1 = no_booking_engine (rejected)
#    0 = pending (in pipeline)
#    1 = launched (live lead)

# =============================================================================
# LOCAL WORKFLOWS (run on your machine)
# =============================================================================
local:
  # Step 1: Ingest region polygons (one-time per state)
  ingest_regions:
    description: Fetch city polygons from OSM and save to database
    command: uv run python -m workflows.ingest_regions --state FL
    # Run once per state to set up scraping regions

  # Step 2: Scrape hotels in polygon regions
  scrape_polygon:
    description: Scrape hotels from Serper for polygon regions
    command: uv run python -m workflows.scrape_polygon --state FL
    # Scrapes all ingested regions for a state
    # Use --only "Miami" "Orlando" to scrape specific cities
    # Use --estimate to preview cost before scraping

  # Step 3: Deduplicate scraped hotels
  deduplicate:
    description: Mark duplicate hotels (placeId → location → name)
    command: uv run python -m workflows.deduplicate
    # Use --dry-run to preview, --stats for stats only

  # Step 4: Queue hotels for detection
  enqueue_detection:
    description: Queue scraped hotels to SQS for detection workers
    command: uv run python workflows/enqueue_detection.py --limit 500

  # Step 5: Export final leads
  export_all:
    description: Export all states to Excel and upload to S3
    command: uv run python workflows/export.py --all
    # Exports all states with hotels

  export_state:
    description: Export single state leads to Excel
    command: uv run python workflows/export.py --state FL
    # Use --state FL for full state export
    # Use --city "Miami" --state FL for single city
    # Use --source dbpr to filter by source

  export_crawl:
    description: Export crawl data by booking engine
    command: uv run python -m workflows.export_crawl --all
    # Exports all booking engines (cloudbeds, mews, siteminder, rms)
    # Use --engine cloudbeds for single engine

  # Utility: Check pipeline status
  status:
    description: Check pipeline status (pending, processing, launched counts)
    command: uv run python workflows/launcher.py status

# =============================================================================
# EC2 WORKFLOWS (auto-start on instance boot via systemd/cron)
# =============================================================================
# These run automatically when you start EC2 instances
# Configure once in AMI, then just start/stop instances as needed
ec2:
  detection_consumer:
    description: Poll SQS and process hotel detection (parallelizable)
    command: uv run python workflows/detection_consumer.py --concurrency 6
    type: systemd  # Runs continuously while instance is up

  enqueue_detection:
    description: Auto-queue pending hotels for detection
    command: uv run python workflows/enqueue_detection.py --limit 500 --categories hotel motel --no-notify
    schedule: "*/5 * * * *"  # Every 5 min
    type: cron

  enrichment_room_counts:
    description: Enrich hotels with room counts via Groq LLM
    command: uv run python workflows/enrichment.py room-counts --limit 100
    schedule: "*/2 * * * *"  # Every 2 min
    type: cron

  enrichment_proximity:
    description: Calculate nearest existing customer distance
    command: uv run python workflows/enrichment.py proximity --limit 200
    schedule: "*/2 * * * *"  # Every 2 min
    type: cron

  launcher:
    description: Launch fully enriched hotels (mark as live)
    command: uv run python workflows/launcher.py launch-all
    schedule: "*/2 * * * *"  # Every 2 min
    type: cron

  # Booking page enrichment (scrape name/address from booking pages)
  booking_enrichment_consumer:
    description: Poll SQS and enrich hotels from booking pages (with archive fallback)
    command: uv run python -m workflows.enrich_booking_pages_consumer --archive-fallback
    type: systemd  # Runs continuously while instance is up

  enqueue_booking_enrichment:
    description: Queue hotels needing enrichment for workers
    command: uv run python -m workflows.enrich_booking_pages_enqueue --limit 1000
    schedule: "*/10 * * * *"  # Every 10 min
    type: cron
    host: primary  # Only run on first/primary EC2 instance

  # DEPRECATED: Cloudbeds now uses booking_enrichment_consumer (has archive fallback)
  # cloudbeds_enrichment_consumer:
  #   description: Poll SQS and scrape Cloudbeds hotels with Playwright
  #   command: uv run python -m workflows.enrich_cloudbeds_consumer --concurrency 6
  #   type: systemd
  #
  # enqueue_cloudbeds_enrichment:
  #   description: Queue Cloudbeds hotels needing enrichment
  #   command: uv run python -m workflows.enrich_cloudbeds_enqueue --limit 5000
  #   schedule: "*/15 * * * *"
  #   type: cron
  #   host: primary

  # Mews enrichment (scrape hotel name from Mews booking pages)
  mews_enrichment_consumer:
    description: Poll SQS and scrape Mews hotels with Playwright
    command: uv run python -m workflows.enrich_mews_consumer --concurrency 6
    type: systemd

  enqueue_mews_enrichment:
    description: Queue Mews hotels needing enrichment
    command: uv run python -m workflows.enrich_mews_enqueue --limit 3000
    schedule: "*/15 * * * *"  # Every 15 min
    type: cron
    host: primary

  # RMS enrichment (scrape hotel data from RMS booking pages)
  enrich_rms_enqueue:
    description: Enqueue RMS hotels needing enrichment
    command: uv run python workflows/enrich_rms_enqueue.py --limit 1000
    schedule: "0 */4 * * *"  # Every 4 hours
    type: cron
    host: primary

  enrich_rms_consumer:
    description: Scrape RMS booking pages for hotel data (name, email, phone, etc.)
    command: uv run python workflows/enrich_rms_consumer.py --concurrency 6
    type: systemd  # Runs continuously

  # Serper geocoding (for non-Cloudbeds crawl data - SiteMinder, Mews, etc.)
  geocode_crawl_data:
    description: Geocode crawl hotels by name using Serper Places API
    command: uv run python -m workflows.geocode_by_name --source '%crawl%' --limit 500
    schedule: "0 * * * *"  # Every hour
    type: cron
    host: primary  # Only run on primary (costs money)

  # -------------------------------------------------------------------------
  # EXPORTS (scheduled export to S3)
  # -------------------------------------------------------------------------
  export_states:
    description: Export all state reports to S3
    command: uv run python workflows/export.py --all
    schedule: "0 */6 * * *"  # Every 6 hours
    type: cron
    host: primary

  export_crawl:
    description: Export crawl data by booking engine to S3
    command: uv run python -m workflows.export_crawl --all
    schedule: "30 */6 * * *"  # Every 6 hours (offset by 30 min)
    type: cron
    host: primary
